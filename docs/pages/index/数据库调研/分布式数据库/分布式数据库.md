# **分布式数据库**
# **MS SQL AZure**
## **概述**
Azure SQL 数据库属于Azure SQL系列，是为云构建的智能、可缩放的关系数据库服务。它是永久性的且始终保持最新状态，并提供 AI 支持的自动化功能来优化性能和持续性。无服务器计算和超大规模存储选项会自动按需缩放资源，使你可以专注于构建新的应用程序，而不必担心存储大小或资源管理。
## **架构**
![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.001.png) 

Windows Azure SQL Databsae 的基底是SQL Server (通常以最新版为主)，不过它是一个特殊设计的SQL Server，并且以Windows Azure为基座平台，配合Windows Azure的特性，Windows Azure SQL Databsae也是一种分散在许多实体基础架构(Physical Infrastucture)与其内部许多虚拟服务器(Virtual Servers)的一种云存储服务，外部应用程序或服务可以不用在乎数据库实际存储在哪里，就可以利用Windows Azure SQL Databsae显露的SQL Fabric壳层服务以接受外部链接，并且在内部使用连线绕送 (connection routing) 的方式，让连线可以对应到正确的服务器，而且数据库是在云端中由多个服务器来提供服务，每一次连线所提供服务的服务器可能会不同，因此也可以保证云存储的高度可用性(High availability)。

Windows Azure SQL Databsae 架构在数据中心可分为三个部分[\[1\]](https://zh.wikipedia.org/wiki/Microsoft_Azure_SQL_Database#cite_note-1)：
## **服务提供层 (Service Layer)** 
服务提供层是 Windows Azure SQL Database 显露在客户端前面的服务接口 (Facade 模式)，负责接取所有向 Windows Azure SQL Databsae 提交要求的 TDS over SSL 连线与指令，当连线进入 Windows Azure SQL Databsae 时，Windows Azure SQL Databsae Load Balancer 会分派连线到不同的 Windows Azure SQL Databsae Gateway 中。Windows Azure SQL Databsae Gateway系负责处理 TDS 连线，管理连接层安全性 (connection-level security) 以及解析指令是否有内含潜在威胁的指令，再交由连线管理员 (Connection Manager) 将连线分派到位于平台提供层内不同的 Windows Azure SQL Databsae 数据库服务器中进行处理，Windows Azure SQL Databsae Gateway 也会管理对 SQL Azure 的连线，以避免可能会屏蔽住服务器的连线 (例如过长的查询或过长的数据库交易等)。
## **平台提供层 (Platform Layer)** 
平台提供层则是以 Windows Azure Computes 的虚拟机集群 (Cluster)，每台虚拟机都安装有 SQL Server 以及管理一定数量的数据库，通常一份数据库会分散到三至五台的 SQL Server VM 中，而每台 SQL Server VM 也安装了 SQL Fabric 中控软件，并透过 SQL Fabric 与 Windows Azure SQL Databsae Gateway 的管控下，所有对单一数据库的连线都不一定会持续连入同一台 SQL Server VM 中。SQL Server VM 内也安装了 SQL Azure Management Service，它会负责对每个数据库间的资料复写工作，以保障 Windows Azure SQL Databsae 的基本高可用性要求。每台 SQL Server VM 内的 SQL Fabric 和 Management Service 都会彼此交换健康与监控信息等，以保持整体服务的健康与可监控性。
## **基础建设层 (Infrastructure Layer)** 
基础建设层由 Windows Azure Computes 以及其高度可扩展性的运算与网络基础架构来组成，以支持 Windows Azure SQL Databsae 所需的高可用性以及高扩展性等云端特色。
## **特点**
1、[完全托管的](https://docs.microsoft.com/zh-cn/azure/sql-database/sql-database-technical-overview/)数据库可自动执行更新、预配和备份，使你可以专注于应用程序开发。

2、灵活且响应灵敏的无服务器计算和超大规模存储可快速适应不断变化的需求。

3、[保护层](https://docs.microsoft.com/zh-cn/azure/sql-database/sql-database-security-overview/)、内置控件和智能威胁检测可确保数据安全。

4、内置AI和内置的高可用性提供高达 99.995%的SLA，保证峰值性能和持续性。
# **Google Spanner**
## **概述**
Google Spanner 是一个可扩展的、全球分布式的数据库，是在谷歌公司设计、开发和部署的。 在最高抽象层面，Spanner 就是一个数据库，把数据分片存储在许多 Paxos[21]状态机上，这 些机器位于遍布全球的数据中心内。复制技术可以用来服务于全球可用性和地理局部性。客 户端会自动在副本之间进行失败恢复。随着数据的变化和服务器的变化，Spanner 会自动把 数据进行重新分片，从而有效应对负载变化和处理失败。Spanner 被设计成可以扩展到几百 万个机器节点，跨越成百上千个数据中心，具备几万亿数据库行的规模。
## **架构**
![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.002.png)

从高层次来看Spanner，它就是一个将数据分片保存在跨越多个数据中心的Paxos状态机上面的数据库。

一个Spanner的部署被称为一个universe。一个Spanner有一些zones组成，zone是基本的管理部署单元，一个数据中心内可以有多个zone，zone可以被动态添加以及移除。在一个zone里面会有一个zone master，数百到数千个spanserber。Zone master复杂将数据交给spansever保存，spanserver则为客户端提供数据服务。另外，一个zone有一个 location proxies用来是客户端可以获取数据和spanserver之间的关系。在zone的上层，有一个universemaster，pacement driver，它们都只有一个，前者用来显示系统的一些状态信息以及调试用的，后者则是用来和spanserver周期性地交换，发现需要被移动的数据，满足副本的约束(比如某些副本变得不可用了，就需要生成新的副本)，还有就是复杂均衡。
## **特点**
数据副本可以在很细的粒度上进行动态的控制。应用可以控制哪些数据中心保存哪些数据，控制副本之间的距离以及副本的数量。同时，这些数据还可以在数据中心之间动态透明地移动，以便于平衡数据中心内资源的使用。

Spanner还实现了分布式数据库很难实现的一些功能，比如它能提供读写的外部一致性，以及在一个时间戳下面的全球一致性的读。还有就是原子的schema变更，即使在有其它操作的情况下也可以。
# **Alibaba OceanBase**
## **概述**
OceanBase是阿里巴巴为业务高吞吐，高并发，高可用场景而自主研发的通用关系型数据库。其处理峰值记录达到6100万次/秒，单集群最大数据量达到3PB，单表行数达万亿级。助您实现水平扩展、分布式、多副本Paxos，以获得商业数据库的高性能和高可用性。OceanBase的分布式设计理念，可以通过水平扩展普通的PC服务器，以商业数据库1/3的成本构建一个能够满足金融级要求的可靠性和数据一致性的数据库系统。
## **架构**
![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.003.png)

OceanBase由以下几个部分组成：

1、客户端：用户使用OceanBase的方式和MySQL数据库完全相同，支持JDBC、C客户端访问，等等。基于MySQL数据库开发的应用程序、工具能够直接迁移到OceanBase。

2、RootServer：管理集群中的所有服务器，子表数据分布以及副本管理。RootServer一般为一主一备，主备之间数据强同步。

3、UpdateServer：存储OceanBase系统的增量更新数据。UpdateServer一般为一主一备，主备之间可以配置不同的同步模式。部署时，UpdateServer进程和RootServer进程往往共用物理服务器。

4、ChunkServer：存储OceanBase系统的基线数据。基线数据一般存储两份或者三份，可配置。

5、MergeServer：接收并解析用户的SQL请求，经过词法分析、语法分析、查询优化等一系列操作后转发给相应的ChunkServer或者UpdateServer。如果请求的数据分布在多台ChunkServer上，MergeServer还需要对多台ChunkServer返回的结果进行合并。客户端和MergeServer之间采用原生的MySQL通信协议，MySQL客户端可以直接访问MergeServer。

OceanBase支持部署多个机房，每个机房部署一个包含RootServer、MergeServer、ChunkServer以及UpdateServer的完整OceanBase集群，每个集群由各自的RootServer负责数据划分、负载均衡、集群服务器管理等操作，集群之间数据同步通过主集群的主UpdateServer往备集群同步增量更新操作日志实现。客户端配置了多个集群的RootServer地址列表，使用者可以设置每个集群的流量分配比例，客户端根据这个比例将读写操作发往不同的集群。
## **特点**
1、卓越性能：6100万次/秒处理峰值的业内纪录.

支持业务快速的扩容缩容，同时通过准内存处理架构实现高性能，单集群最大数据量超过 3 PB，最大单表行数达万亿级。

2、兼容并包：全面支持多种数据库模式。

同时兼容 MySQL 以及 Oracle 两种模式，原来的代码、应用程序只需做较小的改动就可以直接使用 OceanBase。

3、极致弹性：透明的可扩展能力。

通过分布式事务以及全局时间，实现了计算能力以及存储空间的无限水平扩展，业务无需改动任何一行代码即可实现扩容，解决分库分表烦扰。

4、持续可用：三地五中心容灾架构的倡导者与实践者。

采用 Paxos 协议，通过数据多副本，基于普通 PC 服务器实现主备自动切换，且不丢失一行数据（RPO=0，RTO<30秒），成功抵御单机故障以及可用区风险。

5、久经验证：支付宝、网商银行核心业务的共同选择。

已经在阿里巴巴集团旗下蚂蚁金服所有核心金融场景中稳定运行多年，历经近十年双十一，支撑了一个又一个极限场景的考验。
# **Mysql sharding**
## **基本思想**
Sharding的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。不太严格的讲，对于海量数据的数据库，如果是因为表多而数据多，这时候适合使用垂直切分，即把关系紧密（比如同一模块）的表切分出来放在一个server上。如果表并不多，但每张表的数据非常多，这时候适合水平切分，即把表的数据按某种规则（比如按ID散列）切分到多个数据库(server)上。当然，现实中更多是这两种情况混杂在一起，这时候需要根据实际情况做出选择，也可能会综合使用垂直与水平切分，从而将原有数据库切分成类似矩阵一样可以无限扩充的数据库(server)阵列。
## **垂直切分、水平切分、联合切分**
垂直切分的最大特点就是规则简单，实施也更为方便，尤其适合各业务之间的耦合度非
常低，相互影响很小，业务逻辑非常清晰的系统。在这种系统中，可以很容易做到将不同业
务模块所使用的表分拆到不同的数据库中。根据不同的表来进行拆分，对应用程序的影响也
更小，拆分规则也会比较简单清晰。（这也就是所谓的”share nothing”）。

![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.004.png)

水平切分于垂直切分相比，相对来说稍微复杂一些。因为要将同一个表中的不同数据拆
分到不同的数据库中，对于应用程序来说，拆分规则本身就较根据表名来拆分更为复杂，后期的数据维护也会更为复杂一些。

![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.005.png)

让我们从普遍的情况来考虑数据的切分：一方面，一个库的所有表通常不可能由某一张表全部串联起来，这句话暗含的意思是，水平切分几乎都是针对一小搓一小搓（实际上就是垂直切分出来的块）关系紧密的表进行的，而不可能是针对所有表进行的。另一方面，一些负载非常高的系统，即使仅仅只是单个表都无法通过单台数据库主机来承担其负载，这意味着单单是垂直切分也不能完全解决问明。因此多数系统会将垂直切分和水平切分联合使用，先对系统做垂直切分，再针对每一小搓表的情况选择性地做水平切分。从而将整个数据库切分成一个分布式矩阵。

![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.006.png)
## **切分策略**
切分是按先垂直切分再水平切分的步骤进行的。垂直切分的结果正好为水平切分做好了铺垫。垂直切分的思路就是分析表间的聚合关系，把关系紧密的表放在一起。多数情况下可能是同一个模块，或者是同一“聚集”。这里的“聚集”正是领域驱动设计里所说的聚集。在垂直切分出的表聚集内，找出“根元素”（这里的“根元素”就是领域驱动设计里的“聚合根”），按“根元素”进行水平切分，也就是从“根元素”开始，把所有和它直接与间接关联的数据放入一个shard里。这样出现跨shard关联的可能性就非常的小。应用程序就不必打断既有的表间关联。比如：对于社交网站，几乎所有数据最终都会关联到某个用户上，基于用户进行切分就是最好的选择。再比如论坛系统，用户和论坛两个模块应该在垂直切分时被分在了两个shard里，对于论坛模块来说，Forum显然是聚合根，因此按Forum进行水平切分，把Forum里所有的帖子和回帖都随Forum放在一个shard里是很自然的。

对于共享数据数据，如果是只读的字典表，每个shard里维护一份应该是一个不错的选择，这样不必打断关联关系。如果是一般数据间的跨节点的关联，就必须打断。

需要特别说明的是：当同时进行垂直和水平切分时，切分策略会发生一些微妙的变化。比如：在只考虑垂直切分的时候，被划分到一起的表之间可以保持任意的关联关系，因此你可以按“功能模块”划分表格，但是一旦引入水平切分之后，表间关联关系就会受到很大的制约，通常只能允许一个主表（以该表ID进行散列的表）和其多个次表之间保留关联关系，也就是说：当同时进行垂直和水平切分时，在垂直方向上的切分将不再以“功能模块”进行划分，而是需要更加细粒度的垂直切分，而这个粒度与领域驱动设计中的“聚合”概念不谋而合，甚至可以说是完全一致，每个shard的主表正是一个聚合中的聚合根！这样切分下来你会发现数据库分被切分地过于分散了（shard的数量会比较多，但是shard里的表却不多），为了避免管理过多的数据源，充分利用每一个数据库服务器的资源，可以考虑将业务上相近，并且具有相近数据增长速率（主表数据量在同一数量级上）的两个或多个shard放到同一个数据源里，每个shard依然是独立的，它们有各自的主表，并使用各自主表ID进行散列，不同的只是它们的散列取模（即节点数量）必需是一致的。
## **切分需要关注的问题**
## **事务问题**
解决事务问题目前有两种可行的方案：分布式事务和通过应用程序与数据库共同控制实现事务下面对两套方案进行一个简单的对比。
方案一：使用分布式事务
优点：交由数据库管理，简单有效
缺点：性能代价高，特别是shard越来越多时
方案二：由应用程序和数据库共同控制
原理：将一个跨多个数据库的分布式事务分拆成多个仅处于单个数据库上面的小事务，并通过应用程序来总控各个小事务。
优点：性能上有优势
缺点：需要应用程序在事务控制上做灵活设计。如果使用了spring的事务管理，改动起来会面临一定的困难。
## **跨节点Join的问题**
只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。
## **跨节点的count,order by,group by以及聚合函数问题**
这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。
# **SequoiaDB**
## **概述**
SequoiaDB 巨杉数据库是一款金融级分布式关系型数据库，其自研的原生分布式存储引擎支持完整 ACID，具备弹性扩展、高并发和高可用特性，支持 MySQL、PostgreSQL 和 SparkSQL 等多种 SQL 访问形式，适用于核心交易、数据中台、内容管理等应用场景。
## **架构**
![](/images/fenbushi/Aspose.Words.e155d991-2bba-4b18-8ef6-192bfb238a28.007.png)

SequoiaDB 巨杉数据库存储引擎采用分布式架构。集群中的每个节点为一个独立进程，节点之间采用 TCP/IP 协议进行通讯。

同一个操作系统可以部署多个节点，节点之间采用不同的端口进行区分。

SequoiaDB 巨杉数据库的节点分为三种不同的角色：协调节点、编目节点与数据节点。
## **协调节点**
协调节点不存储任何用户数据。作为外部访问的接入与请求分发节点，协调节点将用户请求分发至相应的数据节点，最终合并数据节点的结果应答对外进行响应。
## **编目节点**
编目节点主要存储系统的节点信息、用户信息、分区信息以及对象定义等元数据。在特定操作下，协调节点与数据节点均会向编目节点请求元数据信息，以感知数据的分布规律和校验请求的正确性。
## **数据节点**
数据节点为用户数据的物理存储节点，海量数据通过分片切分的方式被分散至不同的数据节点。在关系型与 JSON 数据库实例中，每一条记录会被完整地存放在其中一个或多个数据节点中；而在对象存储实例中，每一个文件将会依据数据页大小被拆分成多个数据块，并被分散至不同的数据节点进行存放。
## **特点**
## **标准SQL支持，MySQL协议级兼容**
SequoiaDB目前支持标准SQL的访问，同时还在协议级别完整兼容了MySQL/PostgreSQL的语法。SequoiaDB除了100%兼容行业标准的MySQL、PostgreSQL以及SparkSQL语法及协议外，还提供了类S3对象访问以及Posix文件系统接口、MongoDB兼容的原生JSON引擎以及深度数据压缩等多项全新功能，满足传统应用开发人员对于新一代[分布式数据库](https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/1238109)的结构化、半结构化以及非结构化访问方式的需求。
## **金融级分布式OLTP**
作为一款金融级多模分布式数据库，SequoiaDB 全面支持MySQL与PostgreSQL语法协议兼容的OLTP在线交易业务。SequoiaDB使用其自研的开源数据库存储引擎，全面支持ACID（原子性、一致性、隔离性与持久性）、分布式跨表跨节点事务能力、可配置强一致与最终一致性保证、同时在优化器端支持CBO（Cost-Based Optimization）、多维度数据分区、以及HTAP等多种技术特性。
## **分布式架构**
SequoiaDB数据存储引擎采用原生分布式架构，数据完全打散在分布式节点间存储，自动化数据分布和管理，数据可以按需扩展。巨杉数据库通过原生分布式架构，可以轻松实现PB级别数据管理，目前生产环境实测支持超过1000个节点集群。
## **Multi-Model多模数据引擎**
SequoiaDB灵活的数据存储类型，支持非结构化、结构化和半结构化数据全覆盖，实现多模（Multi-Model）数据统一管理。SequoiaDB采取了多引擎的设计，除了记录引擎还提供了对象存储引擎。多模引擎设计让数据库平台场景更多样，也能符合云数据架构下对于多样化业务数据的统一管理、运维要求。
## **HTAP混合事务/分析处理**
SequoiaDB通过SQL的完全支持以及Spark的整合，实现HTAP混合事务、分析处理，快速实现业务应用的弹性开发，应对更多复杂应用场景。同时，通过分布式数据库多副本机制，可以将在线交易和离线分析业务划分隔离，实现同一组数据在应对不同类型业务时互不干扰。
## **数据安全与多活容灾**
数据安全保障是金融和其他大型企业用户关心的技术功能之一，“两地三中心”甚至“三地五中心”正在成为金融级数据库的基本要求。

巨杉数据库原生支持数据库内核级别的高可用以及跨数据中心灾备能力，目前已经实现异地容灾备份，可满足“三地五中心”的容灾支持。同时，巨杉数据库在异地容灾基础上，实现了数据异地多活。

## **应用场景**
## **分布式在线交易业务**
SequoiaDB 巨杉数据库支持 MySQL 协议级兼容与完整分布式事务，原生支持集群弹性水平扩张，并且能够在分布式架构下灵活调整数据一致性，适合用户提供分布式联机交易业务。 
## **企业级数据中台**
数据中台主要提供全量数据的实时在线服务，同时提供对数据进行采集、计算、存储、加工以及基于全量数据的数据价值发掘和数据科学工程等。
## **内容管理平台**
SequoiaDB 分布式内容管理解决方案提供了可弹性扩张的非结构化数据存储平台，以及包含批次管理、版本管理、生命周期管理、标签管理、模糊检索、断点续传等丰富的元数据管理机制。


